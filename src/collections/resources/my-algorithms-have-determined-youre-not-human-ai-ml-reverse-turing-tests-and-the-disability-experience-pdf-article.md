---
title: "My Algorithms Have Determined Youâ€™re Not Human: AI-ML, Reverse Turing-Tests, and the Disability Experience"
focus: "AI and Disability/Outliers"
source: "ASSETS 2019"
readability: ["E"]
type: "PDF Article"
toolPurpose: []
toolAccessibilityIssues: []
openSource: false
sharePointUrl: "https://ocaduniversity.sharepoint.com/teams/Team_WeCount/Shared%20Documents/Resources%20and%20Tools/Literature%20(curated)/My%20Algorithms%20Have%20Determined%20You're%20Not%20Human.pdf"
keywords: ["Artificial intelligence","deep neural networks","disabilities","race","bias"]
learnTags: ["bias","dataset","methods","disability","ethics","inclusivePractice","machineLearning","smallData"]
summary: "A keynote presentation from the 2019 ASSETS conference that suggests diversity and inclusion need to introduced at the start of the AI-ML design process, rather than as an afterthought. "
---
The past decade has seen an exponential growth in the capabilities and deployment of artificial intelligence systems based on deep neural networks. These are visible through the speech recognition and natural language processing of Alexa/Siri/Google that structure many of our everyday interactions, and the promise of SAE Level 5 autonomous driving provided by Tesla and Waze. Aside from these shiny and visible applications of AI-ML are many other uses that are more subtle: AI-ML is now being used to screen job applicants as well as determine which web ads we are shown. And while many vendors of AI-ML technologies have promised that these tools provide for greater access and freedom from human prejudice, disabled users have found that these tools can embed and deploy newer, subtler forms of discrimination against disabled people. At their worst, AIML systems can deny disabled people their humanity.
