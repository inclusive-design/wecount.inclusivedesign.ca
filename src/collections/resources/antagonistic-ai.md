---
title: Antagonistic AI
focus: Methods or Design
source: arXiv
readability:
  - Expert
type: PDF Article
openSource: true
link: https://arxiv.org/pdf/2402.07350.pdf
keywords:
  - antagonism
  - design paradigms
  - human-AI interaction
  - language models
  - AI ethics
  - human-AI collaboration
learnTags:
  - machineLearning
  - methods
  - ethics
  - researchCentre
  - fairness
summary: Researchers explore antagonistic AI — AI systems that are disagreeable,
  rude, interrupting, confrontational, challenging, etc. —embedding opposite
  behaviours or values. They consider whether antagonistic AI systems may
  sometimes have benefits to users, such as forcing users to confront their
  assumptions, build resilience, or develop healthier relational boundaries.
---
The vast majority of discourse around AI development assumes that subservient, “moral” models aligned with “human values” are universally beneficial —in short, that good AI is sycophantic AI. We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc.—embedding opposite behaviors or values. Far from being “bad” or “immoral,” we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries. Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience. Finally, we discuss the many ethical challenges of this space and identify three dimensions for the responsible design of antagonistic AI—consent, context, and framing.
