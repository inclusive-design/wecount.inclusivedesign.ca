---
title: Many-Shot Jailbreaking
focus: Methods or Design
source: Anthropic
readability:
  - Expert
type: Website Article
openSource: true
link: https://www.anthropic.com/research/many-shot-jailbreaking
learnTags:
  - machineLearning
  - ethics
  - methods
summary: Anthropic researchers have identified a new jailbreaking technique
  called "many-shot jailbreaking," which can be used to evade LLM safety
  guardrails. By taking advantage of the increased context window and including
  large amounts of scripted text, LLMs can be forced to produce potentially
  harmful responses that go against their training, such as explaining how to
  build a bomb.
---
