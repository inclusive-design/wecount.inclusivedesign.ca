---
title: Unintended Machine Learning Biases as Social Barriers for Persons with
  Disabilities
focus: AI and Disability/Outliers
source: CHI 2016
readability:
  - Expert
type: Website Article
openSource: false
sharePointUrl: https://ocaduniversity.sharepoint.com/teams/Team_WeCount/Shared%20Documents/Resources%20and%20Tools/Literature%20(curated)/Unintended%20machine%20learning%20biases%20as%20social%20barriers%20for%20persons%20with%20disabilities.pdf
link: https://research.google/pubs/pub48613/
keywords: []
learnTags:
  - bias
  - disability
  - fairness
  - machineLearning
summary: "A research paper about barriers and issues of fairness faced by
  persons with disabilities due to the social biases present in machine learning
  natural language processing models.  "
---
Persons with disabilities face many barriers to full participation in society, and the rapid advancement of technology has the potential tocreate ever more. Building equitable and inclusive technologies for people with disabilities demands paying attention to more than accessibility, but also to how social attitudes towards disability are represented within technology. Representations perpetuated by machine learning (ML) models often inadvertently encode undesirable social biases from the data on which they are trained. This can result, for example, in text classiﬁcation models producing very different predictions for I am a person with mental illness, and I am a tallperson. In this paper,we present evidence of such biases in existing ML models, and in data used for model development. First, we demonstrate that a machine-learned model to moderate conversations classiﬁes texts which mention disability as more “toxic”. Similarly, a machine-learned sentiment analysis model rates texts which mention disability as more negative. Second,we demonstrate that neural text representation models that are critical to many ML applications can also contain undesirable biases towards mentions of disabilities. Third, we show that the data used to develop such models reﬂects topical biases in social discourse which may explain such biases in the models—for instance, gun violence, homelessness, and drug addiction are over-represented in discussions about mental illness
